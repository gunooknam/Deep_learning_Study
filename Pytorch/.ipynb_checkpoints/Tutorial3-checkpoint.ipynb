{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch의 텐서(Tensor) 자료형은 Numpy의 배열과 유사한 자료형이다. 텐서 자료형을 사용하는 방법도 Numpy와 비슷하다. 텐서 자료형 데이터를 만드는 방법은 3가지가 있다.\n",
    "\n",
    "```\n",
    "* 리스트나 Numpy 배열을 텐서로 변환\n",
    "* 0 또는 1 등의 특정한 값을 가진 텐서를 생성\n",
    "* 랜덤한 값을 가지는 텐서를 생성\n",
    "\n",
    "리스트를 텐서 자료형으로 바꾸려면 torch.tensor() 또는 torch.as_tensor(), torch.from_numpy() 명령쓴다.\n",
    "torch.tensor(): 값 복사로 새로운 텐서 자료형 인스턴스 생성\n",
    "torch.as_tensor() : 리스트나 ndarray 객체를 받는다. 값 참조를 사용하여 텐서 자료형 뷰를 만든다.\n",
    "torch.from_numpy() : ndarray 객체를 받는다. 값 참조(refernce)를 사용하여 텐서 자료형 뷰(view)를 만든다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "li= np.array([[1,2], [3,4]])\n",
    "li_tensor = torch.tensor(li)\n",
    "li_as_tensor = torch.as_tensor(li)\n",
    "print(type(li))\n",
    "print(type(li_tensor))\n",
    "print(type(li_as_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32) torch.int32\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32) torch.int32\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32) torch.int32\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "arr_tensor = torch.tensor(arr)\n",
    "arr_as_tensor = torch.from_numpy(arr)\n",
    "arr_from_numpy = torch.from_numpy(arr)\n",
    "\n",
    "print(arr_tensor, arr_tensor.dtype)\n",
    "print(arr_as_tensor, arr_as_tensor.dtype)\n",
    "print(arr_from_numpy, arr_from_numpy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반대는 torch.numpy()를 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(arr_tensor.numpy())\n",
    "print(arr_as_tensor.numpy())\n",
    "print(arr_from_numpy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.as_tensor()나 torch.from_numpy()는 원래의 ndarray 객체를 참조하므로 원래 ndarray 객체의 값을 바꾸면 텐서 자료형의 값도 바뀌고 반대로 텐서 자료형에서 원소의 값을 바꾸면 원래 ndarray 객체의 값도 바뀐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    2]\n",
      " [   3    4]]\n"
     ]
    }
   ],
   "source": [
    "arr_as_tensor[0, 0] = 1000\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8909, 0.2480, 0.9030, 0.5746, 0.3453])\n",
      "tensor([ 0.3456,  3.0559, -1.2300,  0.0677,  0.6185])\n",
      "tensor([8., 3., 0., 5., 4.])\n",
      "tensor([2, 1, 3, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5) # 0과 1사이의 숫자를 균등하게 생성\n",
    "b = torch.randn(5) # 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n",
    "c = torch.randint(10, size=(5,)) # 주어진 범위 내의 정수를 균등하게 생성, 자료형은 torch.float32\n",
    "d = torch.randperm(5) # 랜덤하게 주어진 정수 범위내 값 생성\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a=torch.zeros_like(arr_as_tensor) # 사이즈를 튜플로 입력하지 않고 기존의 텐서로 사용한다.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 10, 5) # 시작점과 끝점을 주어진 갯수만큼 로그간격으로 나눈 간격점을 행벡터로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 자료형을 변환하기\n",
    "## .type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_tensor.type(dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서의 형상 변환\n",
    "### 차원을 늘리거나 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(4, 3)\n",
    "t2 = t1.view(3, 4)\n",
    "t3 = t1.view(12)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.view(1, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.rand(1, 3, 3)\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0488, 0.1821, 0.5508],\n",
      "        [0.1818, 0.1421, 0.0397],\n",
      "        [0.6655, 0.2508, 0.4561]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t4.squeeze())  # => 차원의 원소가 1인 차원을 없애준다. \n",
    "print(t4.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = torch.rand(3, 3)\n",
    "t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.unsqueeze(0).shape # 인수로 받은 위치에 새로운 차원을 삽입한다. !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squeeze -> 차원의 원소가 1인 차원을 없애주고, \n",
    "# unsqueeze -> 인수로 받은 위치에 새로운 차원을 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 복수의 텐서를 결합\n",
    "### torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 3) \n",
    "b = torch.zeros(3, 3) # 0이니까 0부분 2,와 3이 합쳐져서 5가 된다. \n",
    "\n",
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8923, 0.8704, 0.4100, 0.1225, 0.9535, 0.1595],\n",
      "        [0.6054, 0.6866, 0.1259, 0.8436, 0.6151, 0.6593],\n",
      "        [0.1438, 0.1213, 0.4354, 0.7389, 0.9196, 0.8808]])\n",
      "tensor([[0.8923, 0.8704],\n",
      "        [0.6054, 0.6866],\n",
      "        [0.1438, 0.1213]])\n",
      "tensor([[0.4100, 0.1225],\n",
      "        [0.1259, 0.8436],\n",
      "        [0.4354, 0.7389]])\n",
      "tensor([[0.9535, 0.1595],\n",
      "        [0.6151, 0.6593],\n",
      "        [0.9196, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(3, 6)\n",
    "c1, c2, c3 = torch.chunk(c, 3, dim=1) # dimension 1부분을 2,2,2 짜름\n",
    "print(c)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8923, 0.8704, 0.4100],\n",
      "        [0.6054, 0.6866, 0.1259],\n",
      "        [0.1438, 0.1213, 0.4354]])\n",
      "tensor([[0.1225, 0.9535, 0.1595],\n",
      "        [0.8436, 0.6151, 0.6593],\n",
      "        [0.7389, 0.9196, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "c1, c2 = torch.split(c, 3, dim=1)\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4]) tensor([1, 2, 3, 4, 5])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 5)\n",
    "z = torch.arange(1, 6)\n",
    "print(x,z)\n",
    "print(x + z)\n",
    "print(torch.add(x, z))\n",
    "print(x.add(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인플레이스 연산\n",
    "##  명령어 뒤에 _를 붙이면 자기 자신의 값을 바꾸는 인플레이스 명령임\n",
    "## 인플레이스 명령은 연산 결과를 반환하면서 동시에 자기 자신의 데이터 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 5)\n",
    "z = torch.arange(1, 6)\n",
    "\n",
    "print(x)\n",
    "print(x.add_(z))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1개의 원소를 가진 Tensor를 Python의 Scalar로 만들 때는 .item()함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scl = torch.tensor(1)\n",
    "\n",
    "print(scl.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 사용\n",
    "* GPU를 사용할 수 있는 지 확인하려면 torch.cuda.is_available()을 실행 시켜보면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") # 디바이스 객체를 입력하거나 문자열을 입력하면 된다.\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9821, 0.4438, 0.3672],\n",
       "        [0.1115, 0.3228, 0.8629],\n",
       "        [0.1579, 0.8791, 0.2993]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.rand(3,3,device=device)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9359, 0.6611, 0.5371],\n",
       "        [0.2991, 0.5317, 0.9188],\n",
       "        [0.0160, 0.1156, 0.6885]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.rand(3,3,device=\"cuda:0\")\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5253, 0.8576, 0.2935],\n",
       "        [0.4043, 0.6494, 0.2830],\n",
       "        [0.9057, 0.4642, 0.6351]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = torch.rand(3, 3)\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5253, 0.8576, 0.2935],\n",
       "        [0.4043, 0.6494, 0.2830],\n",
       "        [0.9057, 0.4642, 0.6351]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd \n",
    "* autograd는 Pytorch에서 핵심적인 기능을 담당하는 하부 패키지임\n",
    "* autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능이다. 텐서 자료를 생성할 때,\n",
    "* requires_grad 인수를 True로 설정하고나 .requires_grad_(True)를 실행하면 그 텐서에 행해지는 모든 연산에\n",
    "* 대해서 미분값을 계산한다. \n",
    "* 계산을 멈추고 싶으면 .detach() gkatnfmf dldydgksek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0129, 0.6226],\n",
      "        [0.1725, 0.3476]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4668, grad_fn=<SumBackward0>) <SumBackward0 object at 0x000001ECF9D05358>\n"
     ]
    }
   ],
   "source": [
    "y = torch.sum(x * 3)\n",
    "print(y, y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward()함수는 자동으로 미분값을 계산해 requires_grad인수가 True로 설정된 변수의 grad속성의 값을 갱신한다.\n",
    "\n",
    "# retain_graph 미분을 연산하기 위해서 사용했던 임시 그래프를 유지 할 것인가를 설정\n",
    "\n",
    "# 기본값은 False로 설정되어 있지만 동일한 연산에 대해 여러번 미분을 계산하기 위해서는 True로 설정되어 있어야한다.\n",
    "\n",
    "# 미분값을 그대로 출력받아 사용하고 싶은 경우에는 torch.autograd.grad()함수에 출력값과 입력값을 입력하면 미분값을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "상황에 따라서 특정 연산에는 미분값을 계한하고 싶지 않은 경우 .detach() 함수를 사용한다.예를 들어, 이전 코드의 결과 값 y에 로지스틱 함수 연산을 수행하고 이에 대한 미분 값을 계산 하고 싶지 않은 경우에 다음처럼 할 수 있다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9697)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = y.detach()\n",
    "torch.sigmoid(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20., 20., 40., 40.],\n",
      "        [30., 30., 50., 50.]]) tensor([[30., 30., 50., 50.],\n",
      "        [40., 40., 70., 70.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor( np.array([[20,20,40,40], [30,30,50,50]]), dtype=torch.float32)\n",
    "b=torch.tensor( np.array([[30,30,50,50], [40,40,70,70]]), dtype=torch.float32)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "a = torch.cuda.ByteTensor(4,4).fill_(0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "b = torch.cuda.FloatTensor(4,4).fill_(0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH의 MAX!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 8, 2],\n",
      "        [5, 6, 7, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[0, 1, 2, 8, 2],\n",
    "                [5, 6, 7, 1, 2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8, 7]), tensor([3, 2]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 즉 0번째 Tensor : max의 값, 1번째 Tensor: max 값에 해당하는 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [2, 1, 2, 3, 4],\n",
      "        [1, 6, 7, 8, 6],\n",
      "        [9, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 5],\n",
      "        [4, 1, 2, 3, 4],\n",
      "        [8, 6, 7, 8, 2]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "e=torch.tensor([[0, 1, 2, 3, 4],\n",
    "        [4, 6, 7, 8, 2],\n",
    "        [2, 1, 2, 3, 4],\n",
    "        [1, 6, 7, 8, 6],\n",
    "        [9, 1, 2, 3, 4],\n",
    "        [5, 6, 7, 8, 5],\n",
    "        [4, 1, 2, 3, 4],\n",
    "        [8, 6, 7, 8, 2]], dtype=torch.int32)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uint8로 Tensor로 할경우 index masking이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 1  tensor([[0, 1, 2, 3, 4],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [2, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 5],\n",
      "        [4, 1, 2, 3, 4],\n",
      "        [8, 6, 7, 8, 2]], dtype=torch.int32)\n",
      "case 2  tensor([[4, 6, 7, 8, 2],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [4, 6, 7, 8, 2],\n",
      "        [4, 6, 7, 8, 2]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "c=torch.tensor([1,1,1,0,0,1,1,1], dtype=torch.uint8) # index masking !\n",
    "c2=torch.tensor([1,1,1,0,0,1,1,1]) # index extract !\n",
    "print(\"case 1 \", e[c])\n",
    "print(\"case 2 \", e[c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5]]\n",
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "List = []\n",
    "temp  = [1,2,3,4,5]\n",
    "List += [temp]\n",
    "print(List)\n",
    "List += [temp]\n",
    "print(List)\n",
    "List += [temp]\n",
    "print(List)\n",
    "List += [temp]\n",
    "print(List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.repeat\n",
    "해당 횟수 만큼 반복하는 것인데 이것을 어느 방향으로 하느냐에 따라 다르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "size=13\n",
    "t=torch.arange(size)\n",
    "print(t)\n",
    "z=t.repeat(13,1) # 차원이 하나 늘어난다. !\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], device='cuda:0')\n",
      "torch.Size([200, 200, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.cuda.FloatTensor(200,200,3).fill_(1)\n",
    "print(x)\n",
    "x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.cuda.FloatTensor(3,3).fill_(1)\n",
    "print(a)\n",
    "b=torch.arange(9)\n",
    "b= b.view_as(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "test = (b.view(1,9).permute(1,0))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 1., 2., 3., 4.],\n",
      "          [1., 2., 3., 4., 5.],\n",
      "          [2., 3., 4., 5., 6.],\n",
      "          [3., 4., 5., 6., 7.],\n",
      "          [4., 5., 6., 7., 8.]]]], device='cuda:0')\n",
      "0.001994609832763672\n"
     ]
    }
   ],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor \n",
    "g=5\n",
    "grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "#print(grid_x)\n",
    "#print(grid_y)\n",
    "#print(grid_y.data)\n",
    "\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "print(grid_x+grid_y)\n",
    "endTime = time.time() - startTime\n",
    "print(endTime)\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "print(grid_x.data+grid_y.data)\n",
    "endTime = time.time() - startTime\n",
    "print(endTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
