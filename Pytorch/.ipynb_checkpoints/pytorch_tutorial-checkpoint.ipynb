{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60분 파이토치 코스 돌파하기\n",
    "\n",
    "* Tensor는 pytorch의 자료형이다.\n",
    "* Tensor 변수 뒤에 .cuda()를 추가하면 GPU 연산을 할 수 있다.\n",
    "\n",
    "* tensor type의 종류(cuda를 쓸라면 사이에 .cuda를 붙이면 된다.)\n",
    " * torch.FloatTensor => 32bit float\n",
    " * torch.DoubleTensor => 64bit floating point\n",
    " * torch.HalfTensor => 16-bit floating point\n",
    " * torch.ByteTensor => 8 bit integer (unsigned)\n",
    " * torch.CharTensor => 8 bit integer (signed) \n",
    " * torch.ShortTensor => 16 bit integer()\n",
    " * torch.IntTensor => 32 bit integer()\n",
    " * torch.LongTensor => 64 bit integer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기화 되지 않은 Tensor를 생성할 수 있다.\n",
    "매우 크게도 만들 수 있긴 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8391e+14, 4.5912e-41, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.5656e+28, 1.6635e+22, 1.3563e-19, 1.3563e-19, 1.8578e-01],\n",
      "        [2.7254e+20, 1.6825e+08, 1.8977e+28, 2.8542e+32, 4.5840e+30],\n",
      "        [1.1161e+04, 7.2076e+31, 5.2943e-14, 1.3556e-19, 1.3563e-19],\n",
      "        [3.9592e-11, 1.3563e-19, 1.5222e+31, 1.8465e+25, 7.2708e+31],\n",
      "        [1.0999e-32, 1.3563e-19, 1.3563e-19, 1.4754e-19, 1.3563e-19]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5,5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5,5, dtype=torch.double)\n",
    "print(x)\n",
    "x= torch.randn_like(x, dtype =  torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5685, -1.0906, -0.7811,  1.1093,  1.2782],\n",
      "        [ 0.2705,  1.3708, -0.1593, -0.0919, -0.9147],\n",
      "        [ 0.9828, -0.3660, -0.6122, -1.0627,  0.1999],\n",
      "        [ 0.3273, -0.4657,  0.0850,  0.3009,  2.3634],\n",
      "        [ 0.2645, -0.1243,  0.5556,  0.9868, -1.6898]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0433, 0.7217, 0.0523],\n",
      "        [0.5614, 0.3532, 0.8421],\n",
      "        [0.1806, 0.7400, 0.4407],\n",
      "        [0.3010, 0.3688, 0.7108],\n",
      "        [0.8323, 0.5503, 0.6149]])\n"
     ]
    }
   ],
   "source": [
    "#x = torch.ones(5,3, dtype=torch.double)\n",
    "y = torch.rand(5,3)\n",
    "#print(x + y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# randn(uniform distribution)\n",
    "\n",
    "# rand(normal distribution)\n",
    "평균 0이고 분산이 1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1548, 1.0161, 0.0961],\n",
      "        [0.7310, 0.6772, 1.7250],\n",
      "        [0.4611, 0.8777, 1.0180],\n",
      "        [0.3990, 1.2013, 1.4910],\n",
      "        [1.4560, 1.4295, 1.0046]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1548, 1.0161, 0.0961],\n",
      "        [0.7310, 0.6772, 1.7250],\n",
      "        [0.4611, 0.8777, 1.0180],\n",
      "        [0.3990, 1.2013, 1.4910],\n",
      "        [1.4560, 1.4295, 1.0046]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1548, 1.0161, 0.0961],\n",
      "        [0.7310, 0.6772, 1.7250],\n",
      "        [0.4611, 0.8777, 1.0180],\n",
      "        [0.3990, 1.2013, 1.4910],\n",
      "        [1.4560, 1.4295, 1.0046]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result) # output 텐서를 제공한다.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1548, 1.0161, 0.0961],\n",
       "        [0.7310, 0.6772, 1.7250],\n",
       "        [0.4611, 0.8777, 1.0180],\n",
       "        [0.3990, 1.2013, 1.4910],\n",
       "        [1.4560, 1.4295, 1.0046]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1548, 1.0161, 0.0961],\n",
      "        [0.7310, 0.6772, 1.7250],\n",
      "        [0.4611, 0.8777, 1.0180],\n",
      "        [0.3990, 1.2013, 1.4910],\n",
      "        [1.4560, 1.4295, 1.0046]])\n"
     ]
    }
   ],
   "source": [
    "print(y) # 인플레이스라 y에 저장이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0161, 0.6772, 0.8777, 1.2013, 1.4295])\n"
     ]
    }
   ],
   "source": [
    "print(y[:,1]) # 인덱싱이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([-0.9029,  0.4515,  1.9776, -0.2257, -0.0447,  0.8621, -0.1603, -0.2027,\n",
      "         0.9931, -0.8053,  1.0364,  0.6674, -0.6850, -0.9591, -0.9137, -0.0821])\n",
      "tensor([[-0.9029,  0.4515,  1.9776, -0.2257, -0.0447,  0.8621, -0.1603, -0.2027],\n",
      "        [ 0.9931, -0.8053,  1.0364,  0.6674, -0.6850, -0.9591, -0.9137, -0.0821]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16) # 리사이즈 된 것을 볼 수 있다. !!\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1047])\n",
      "0.10471402853727341\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b) # => pytorch -> numpy로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3., 3.])\n",
      "[3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) # add_는 인플레이스이다. \n",
    "print(a)\n",
    "print(b) # 포인팅이라 이렇게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) # numpy를 torch로 만든다.\n",
    "np.add(a, 1, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy 에서 Tensor로 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy에서 tensor로의 변환이 쉽다\n",
    "a = np.array([1,2,3,4])\n",
    "b=torch.Tensor(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 에서 numpy로 바꾼다\n",
    "numpy() 라는 함수를 붙이자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)\n",
    "b = a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor의 형태 변환 (view)\n",
    "view라는 함수를 쓰면 형태가 변환되는 reshape을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8147, 0.9758, 0.9117],\n",
       "          [0.3611, 0.4720, 0.6365],\n",
       "          [0.1605, 0.0747, 0.6109]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "a = a.view(1,1,3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 합치기\n",
    ".cat 이라는 함수쓰면 된다.\n",
    "\n",
    "특정 차원에 맞게 합쳐주는데...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1234,  0.1742,  0.9533],\n",
      "          [-1.1653, -0.7513, -0.0990],\n",
      "          [-0.9736, -0.0367,  0.3558]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6969,  2.0635,  0.0827],\n",
      "          [ 0.1668, -1.3438,  0.6416],\n",
      "          [-0.9178,  1.0994, -0.5288]]]])\n",
      "tensor([[[[-1.1234,  0.1742,  0.9533],\n",
      "          [-1.1653, -0.7513, -0.0990],\n",
      "          [-0.9736, -0.0367,  0.3558]],\n",
      "\n",
      "         [[ 1.6969,  2.0635,  0.0827],\n",
      "          [ 0.1668, -1.3438,  0.6416],\n",
      "          [-0.9178,  1.0994, -0.5288]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,1,3,3)\n",
    "b = torch.randn(1,1,3,3)\n",
    "c = torch.cat((a,b),0)\n",
    "print(c)\n",
    "c = torch.cat((a,b),1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 계산을 GPU로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1066, -0.5052, -3.1178],\n",
      "        [ 0.0940, -0.4305,  0.0238],\n",
      "        [ 2.1223, -3.9562,  1.8809]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "y = torch.randn(3,3)\n",
    "if torch.cuda.is_available():\n",
    "    x=x.cuda()\n",
    "    y=y.cuda()\n",
    "    sum = x+y\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5218)\n",
      "tensor(4.6958)\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,3)\n",
    "print(a.mean())\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1047], device='cuda:0')\n",
      "tensor([1.1047], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "\n",
    "if torch.cuda.is_available():              \n",
    "    device = torch.device(\"cuda\")          # a CUDA device object \n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: Automatic Differentiation\n",
    "미분을 자동으로 계산하는데 자동으로 계산하는데 사용하는 변수는\n",
    "torch.autograd에 있는 Variable을 사용해야 동작이 가능해진다.\n",
    "automatic differentiation 를 제공하는 기능이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward object at 0x00000245C13A2B00>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베리어블의 선언은 이렇게 하믄 댐\n",
    "\n",
    "autograd.Variable 은 세가지로 나뉜다. => data, grad, grad_fn\n",
    "\n",
    "* data => Tensor 형태의 데이터가 담긴다.\n",
    "\n",
    "* grad => 데이터가 거쳐온 레이어에 대한 미분값이 축적된다.\n",
    "\n",
    "* grad_fn 미분 값을 계산한 함수에 대한 정보(어떤 연산에 대한 backward를 진행했다.)\n",
    "\n",
    "# 버전이 바뀌어서 Variable을 선언하지 않아도 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "#a = Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(2,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(2,2, requires_grad=True) \n",
    "#=> 의마하는 것은 a값이 그라디언트 값이 필요하다라는 내용\n",
    "print(a)\n",
    "# RNN이나 CNN에 들어가는 weight 값은 requires값이 트루라고 선언댐\n",
    "# 근데 input으로 적용되는 a값은 requires_grad=True를 하지 않으면 얘가 그라디언트가\n",
    "# 없는 애구나를 인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........a.data........\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "........a.grad........\n",
      "None\n",
      "........a.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 지금은 아무런 연산이 없어서 그렇다.\n",
    "print(\"........a.data........\")\n",
    "print(a.data)\n",
    "print(\"........a.grad........\")\n",
    "print(a.grad)\n",
    "print(\"........a.grad_fn........\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = a+2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b**2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out=c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 out값을 토대로 backword를 진행을 해주자\n",
    "#print(out)\n",
    "out.backward()\n",
    "# 의미가 무었이냐?\n",
    "# a->b->c 를 만든다. c->out을 만든다. 그려면 \n",
    "# dout/da 를 만드는데 => a.grad인데 \n",
    "# 이전에 a.grad는 None이었는데 이것을 채우기 위해서 해주는 함수다\n",
    "# backward()라고 보면 된다.\n",
    "# 그래서 이걸 수행하면 내용을 채워준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일하게 출력을 했을 때 a.data는 그대로 있고 \n",
    "# 저 grad가 채워진다. \n",
    "# grad_fn은 a가 직접적으로 수행하는 연산이 없어서 직접적으로 계산이 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........a.data........\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "........a.grad........\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "........a.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 지금은 아무런 연산이 없어서 그렇다. => grad_fn은 안참\n",
    "print(\"........a.data........\")\n",
    "print(a.data)\n",
    "print(\"........a.grad........\")\n",
    "print(a.grad)\n",
    "print(\"........a.grad_fn........\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........b.data........\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "........b.grad........\n",
      "None\n",
      "........b.grad_fn........\n",
      "<AddBackward object at 0x00000245C13AE940>\n"
     ]
    }
   ],
   "source": [
    "# 애는 그라디언트가 필요하지 않아서 None이다.\n",
    "# 얘는 대신에 grad_fn이 있는데 채워진 내용 보면 addBackward 제로다.\n",
    "# b가 a+2 이라 add 연산에 대한 backward를 했다.\n",
    "print(\"........b.data........\")\n",
    "print(b.data)\n",
    "print(\"........b.grad........\")\n",
    "print(b.grad)\n",
    "print(\"........b.grad_fn........\")\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........c.data........\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "........c.grad........\n",
      "None\n",
      "........c.grad_fn........\n",
      "<PowBackward0 object at 0x00000245C13AE940>\n"
     ]
    }
   ],
   "source": [
    "# C는 파워 연산을 해서 그렇다\n",
    "# powerBackward 제로다\n",
    "\n",
    "print(\"........c.data........\")\n",
    "print(c.data)\n",
    "print(\"........c.grad........\")\n",
    "print(c.grad)\n",
    "print(\"........c.grad_fn........\")\n",
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........out.data........\n",
      "tensor(36.)\n",
      "........out.grad........\n",
      "None\n",
      "........out.grad_fn........\n",
      "<SumBackward0 object at 0x00000245C13AE7B8>\n"
     ]
    }
   ],
   "source": [
    "# out은 여기다가 Sum에 대한 backward를 진행했다고 담긴다.\n",
    "print(\"........out.data........\")\n",
    "print(out.data)\n",
    "print(\"........out.grad........\")\n",
    "print(out.grad)\n",
    "print(\"........out.grad_fn........\")\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "# z = 3xX^2 다. \n",
    "# round x분에 round z는 6xX다. 거기다 x가 1일 때 6이지 않나 싶다.\n",
    "x=torch.ones(3, requires_grad=True)\n",
    "y = (x**2)\n",
    "z = y*3\n",
    "print(z)\n",
    "grad = torch.Tensor([0.1,1,10]) \n",
    "# 그 이유는 여기다. 텐서를 벡워드 칸에 맞춰서 넣어주면\n",
    "# 벡워드 되는 이 값에 곱해져서 채워지게 된다.\n",
    "z.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........x.data........\n",
      "tensor([1., 1., 1.])\n",
      "........x.grad........\n",
      "tensor([ 0.6000,  6.0000, 60.0000])\n",
      "........x.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"........x.data........\")\n",
    "print(x.data)\n",
    "print(\"........x.grad........\")\n",
    "print(x.grad)\n",
    "print(\"........x.grad_fn........\")\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn, torch.nn.functional 에 대해 알아보자\n",
    "\n",
    "* nn 모듈에서 제공하는 기능\n",
    " * parameter, Linear, Containers, Dropout, Conv, Sparse, Pooling, Distance, Loss, Padding, Vision, Non-linear Activation, Data paralell, Normalization, Utilities\n",
    " , Recurrent\n",
    " \n",
    "* nn.functional 에서 제공하는 기능들은\n",
    " * Conv, Pooling, Non-linear activation, Normalization, Linear function, Dropout, Distance, Loss, Vision 등이 있다.\n",
    " \n",
    " \n",
    "## 두개가 지원하는 기능은 유사한데 사용하는 방식의 차이가 있다.\n",
    "\n",
    "### 그러면 nn.Conv2d에 대해 알아보자\n",
    "* torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "* 인풋채널, 아웃풋 채널(=> 이건 사실상 사용하는 필터의 개수)이 있다. \n",
    "* 특징은 Weight값을 직접 선언해주지 않는다. => 나온 변수를 진행하면 컨볼루션 위한 weight를 선언해준다.\n",
    "\n",
    "### nn.functional 에 대해서 보자\n",
    "* torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor\n",
    "* 앞서 nn.con2d에서 input 채널과 out채널을 썻던것과 달리 인풋과 weight 자체를 직접 넣어줘야 한다. 그래서 이거 쓸려면 외부에서 만든 필터를 직접 만들어서 넣어야 한다.\n",
    "#### 그 외 채워주는 변수들은 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 먼저 F(nn.functional)를 사용한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Functional를 쓰려면 Weight를 선언해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,3,3, requires_grad=True)\n",
    "filter = torch.ones(1,1,3,3) # => 얘가 weight이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(input, filter)\n",
    "print(out) #=> 원하는 형태의 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ThnnConv2DBackward object at 0x00000245C13903C8>\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(out.grad_fn) # Backward 한다라는 것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,3,3, requires_grad=True)\n",
    "filter = filter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[18.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "<ThnnConv2DBackward object at 0x00000245C1390AC8>\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(input, filter)\n",
    "print(out)\n",
    "out.backward()\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----input.grad-----\n",
      "tensor([[[[2., 2., 2.],\n",
      "          [2., 2., 2.],\n",
      "          [2., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"----input.grad-----\")\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그 다음에 nn.Conv2d 사용한 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,3,3, requires_grad=True)\n",
    "func = nn.Conv2d(1,1,3)\n",
    "        # 첫번째 것은 인풋채널, 두번째는 아웃풋 채널, 세번째는 커널 사이즈\n",
    "        # 필터의 크기는 3x3이다. 라는 얘기다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0392,  0.3054,  0.1526],\n",
       "          [ 0.0118,  0.0981, -0.3235],\n",
       "          [-0.2099,  0.1174,  0.1804]]]], requires_grad=True)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
