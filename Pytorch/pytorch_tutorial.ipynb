{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60분 파이토치 코스 돌파하기\n",
    "해당 내용은 https://www.youtube.com/channel/UCK24Wy_G-6V-quKvVRjflgA 에서 참고하였고\n",
    "\n",
    "추가적으로 최건호씨 깃허브 저장소에 가면 더 많은 자료를 볼 수 있음\n",
    "\n",
    "* Tensor는 pytorch의 자료형이다.\n",
    "* Tensor 변수 뒤에 .cuda()를 추가하면 GPU 연산을 할 수 있다.\n",
    "\n",
    "* tensor type의 종류(cuda를 쓸라면 사이에 .cuda를 붙이면 된다.)\n",
    " * torch.FloatTensor => 32bit float\n",
    " * torch.DoubleTensor => 64bit floating point\n",
    " * torch.HalfTensor => 16-bit floating point\n",
    " * torch.ByteTensor => 8 bit integer (unsigned)\n",
    " * torch.CharTensor => 8 bit integer (signed) \n",
    " * torch.ShortTensor => 16 bit integer()\n",
    " * torch.IntTensor => 32 bit integer()\n",
    " * torch.LongTensor => 64 bit integer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기화 되지 않은 Tensor를 생성할 수 있다.\n",
    "매우 크게도 만들 수 있긴 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([                                   0.,\n",
      "                                           0.,\n",
      "        -71104899011676815060077889108049920.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.0961e+37,  7.4549e-43,  7.0065e-44,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.8754e+28,  2.0110e+20,  3.3560e-06,  1.0527e-11,  2.1040e+23],\n",
      "        [ 1.7376e-04,  6.6756e+22,  1.0335e-05,  2.1954e-04,  4.2000e-08],\n",
      "        [ 4.0746e-11,  7.1450e+31,  4.1418e-41,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5,5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5,5, dtype=torch.double)\n",
    "print(x)\n",
    "x= torch.randn_like(x, dtype =  torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4113,  0.0103,  1.3777,  1.1836,  0.5463],\n",
      "        [ 0.6821,  0.6892,  1.1443, -0.9237, -1.1009],\n",
      "        [ 0.1661, -0.6771, -0.0586,  2.3049,  0.1735],\n",
      "        [ 1.6848,  0.4569, -0.0793,  0.2799, -0.6442],\n",
      "        [-0.4479,  0.1146, -1.1878, -0.1855,  0.5525]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9904, 0.8430, 0.9870],\n",
      "        [0.2614, 0.5376, 0.7432],\n",
      "        [0.1388, 0.5484, 0.6546],\n",
      "        [0.9221, 0.1921, 0.4736],\n",
      "        [0.7377, 0.4464, 0.5655]])\n"
     ]
    }
   ],
   "source": [
    "#x = torch.ones(5,3, dtype=torch.double)\n",
    "y = torch.rand(5,3)\n",
    "#print(x + y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# randn(uniform distribution)\n",
    "\n",
    "# rand(normal distribution)\n",
    "평균 0이고 분산이 1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5050, 1.4187, 1.4827],\n",
      "        [0.2754, 0.6187, 1.4884],\n",
      "        [1.1054, 1.1488, 0.9667],\n",
      "        [1.1456, 0.4204, 0.8034],\n",
      "        [1.3563, 1.0537, 1.0434]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5050, 1.4187, 1.4827],\n",
      "        [0.2754, 0.6187, 1.4884],\n",
      "        [1.1054, 1.1488, 0.9667],\n",
      "        [1.1456, 0.4204, 0.8034],\n",
      "        [1.3563, 1.0537, 1.0434]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5050, 1.4187, 1.4827],\n",
      "        [0.2754, 0.6187, 1.4884],\n",
      "        [1.1054, 1.1488, 0.9667],\n",
      "        [1.1456, 0.4204, 0.8034],\n",
      "        [1.3563, 1.0537, 1.0434]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result) # output 텐서를 제공한다.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5050, 1.4187, 1.4827],\n",
       "        [0.2754, 0.6187, 1.4884],\n",
       "        [1.1054, 1.1488, 0.9667],\n",
       "        [1.1456, 0.4204, 0.8034],\n",
       "        [1.3563, 1.0537, 1.0434]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5050, 1.4187, 1.4827],\n",
      "        [0.2754, 0.6187, 1.4884],\n",
      "        [1.1054, 1.1488, 0.9667],\n",
      "        [1.1456, 0.4204, 0.8034],\n",
      "        [1.3563, 1.0537, 1.0434]])\n"
     ]
    }
   ],
   "source": [
    "print(y) # 인플레이스라 y에 저장이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4187, 0.6187, 1.1488, 0.4204, 1.0537])\n"
     ]
    }
   ],
   "source": [
    "print(y[:,1]) # 인덱싱이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([-0.2844, -0.0112, -0.4080, -1.6139, -0.0430, -0.6373,  0.1037,  0.8274,\n",
      "         0.4060, -0.9103,  0.1442, -0.2995, -1.3405,  0.8891,  0.6614, -2.4827])\n",
      "tensor([[-0.2844, -0.0112, -0.4080, -1.6139, -0.0430, -0.6373,  0.1037,  0.8274],\n",
      "        [ 0.4060, -0.9103,  0.1442, -0.2995, -1.3405,  0.8891,  0.6614, -2.4827]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16) # 리사이즈 된 것을 볼 수 있다. !!\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7546])\n",
      "0.7545880675315857\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b) # => pytorch -> numpy로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) # add_는 인플레이스이다. \n",
    "print(a)\n",
    "print(b) # 포인팅이라 이렇게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) # numpy를 torch로 만든다.\n",
    "np.add(a, 1, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy 에서 Tensor로 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy에서 tensor로의 변환이 쉽다\n",
    "a = np.array([1,2,3,4])\n",
    "b=torch.Tensor(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 에서 numpy로 바꾼다\n",
    "numpy() 라는 함수를 붙이자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)\n",
    "b = a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor의 형태 변환 (view)\n",
    "view라는 함수를 쓰면 형태가 변환되는 reshape을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6270, 0.9281, 0.2541],\n",
       "          [0.8286, 0.2717, 0.2023],\n",
       "          [0.3507, 0.1983, 0.0743]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "a = a.view(1,1,3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 합치기\n",
    ".cat 이라는 함수쓰면 된다.\n",
    "\n",
    "특정 차원에 맞게 합쳐주는데...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.2814, -0.7664,  1.4026],\n",
      "          [ 0.3083,  2.2184,  0.3279],\n",
      "          [-0.9566,  0.0263,  0.9656]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9020,  0.5064,  0.5653],\n",
      "          [ 1.9675,  0.1251, -0.2949],\n",
      "          [ 0.1006,  0.1223, -0.9616]]]])\n",
      "tensor([[[[ 1.2814, -0.7664,  1.4026],\n",
      "          [ 0.3083,  2.2184,  0.3279],\n",
      "          [-0.9566,  0.0263,  0.9656]],\n",
      "\n",
      "         [[ 0.9020,  0.5064,  0.5653],\n",
      "          [ 1.9675,  0.1251, -0.2949],\n",
      "          [ 0.1006,  0.1223, -0.9616]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,1,3,3)\n",
    "b = torch.randn(1,1,3,3)\n",
    "c = torch.cat((a,b),0)\n",
    "print(c)\n",
    "c = torch.cat((a,b),1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 계산을 GPU로 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9889, -2.5683, -0.0172],\n",
      "        [ 0.4221,  0.3758, -1.3121],\n",
      "        [ 0.3428,  2.4168, -1.7238]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "y = torch.randn(3,3)\n",
    "if torch.cuda.is_available():\n",
    "    x=x.cuda()\n",
    "    y=y.cuda()\n",
    "    sum = x+y\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4540)\n",
      "tensor(4.0861)\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,3)\n",
    "print(a.mean())\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9772,  0.2928,  0.4032],\n",
      "        [ 1.2558,  0.5644, -0.1179],\n",
      "        [ 1.1717,  2.5108, -0.2582]], device='cuda:0')\n",
      "tensor([[ 1.9772,  0.2928,  0.4032],\n",
      "        [ 1.2558,  0.5644, -0.1179],\n",
      "        [ 1.1717,  2.5108, -0.2582]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "\n",
    "if torch.cuda.is_available():              \n",
    "    device = torch.device(\"cuda\")          # a CUDA device object \n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: Automatic Differentiation\n",
    "미분을 자동으로 계산하는데 자동으로 계산하는데 사용하는 변수는\n",
    "torch.autograd에 있는 Variable을 사용해야 동작이 가능해진다.\n",
    "automatic differentiation 를 제공하는 기능이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward object at 0x0000021481655A90>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베리어블의 선언은 이렇게 하믄 댐\n",
    "\n",
    "autograd.Variable 은 세가지로 나뉜다. => data, grad, grad_fn\n",
    "\n",
    "* data => Tensor 형태의 데이터가 담긴다.\n",
    "\n",
    "* grad => 데이터가 거쳐온 레이어에 대한 미분값이 축적된다.\n",
    "\n",
    "* grad_fn 미분 값을 계산한 함수에 대한 정보(어떤 연산에 대한 backward를 진행했다.)\n",
    "\n",
    "# 버전이 바뀌어서 Variable을 선언하지 않아도 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "#a = Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(2,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(2,2, requires_grad=True) \n",
    "#=> 의마하는 것은 a값이 그라디언트 값이 필요하다라는 내용\n",
    "print(a)\n",
    "# RNN이나 CNN에 들어가는 weight 값은 requires값이 트루라고 선언댐\n",
    "# 근데 input으로 적용되는 a값은 requires_grad=True를 하지 않으면 얘가 그라디언트가\n",
    "# 없는 애구나를 인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........a.data........\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "........a.grad........\n",
      "None\n",
      "........a.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 지금은 아무런 연산이 없어서 그렇다.\n",
    "print(\"........a.data........\")\n",
    "print(a.data)\n",
    "print(\"........a.grad........\")\n",
    "print(a.grad)\n",
    "print(\"........a.grad_fn........\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = a+2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b**2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out=c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 out값을 토대로 backword를 진행을 해주자\n",
    "#print(out)\n",
    "out.backward()\n",
    "# 의미가 무었이냐?\n",
    "# a->b->c 를 만든다. c->out을 만든다. 그려면 \n",
    "# dout/da 를 만드는데 => a.grad인데 \n",
    "# 이전에 a.grad는 None이었는데 이것을 채우기 위해서 해주는 함수다\n",
    "# backward()라고 보면 된다.\n",
    "# 그래서 이걸 수행하면 내용을 채워준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일하게 출력을 했을 때 a.data는 그대로 있고 \n",
    "# 저 grad가 채워진다. \n",
    "# grad_fn은 a가 직접적으로 수행하는 연산이 없어서 직접적으로 계산이 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........a.data........\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "........a.grad........\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "........a.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 지금은 아무런 연산이 없어서 그렇다. => grad_fn은 안참\n",
    "print(\"........a.data........\")\n",
    "print(a.data)\n",
    "print(\"........a.grad........\")\n",
    "print(a.grad)\n",
    "print(\"........a.grad_fn........\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........b.data........\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "........b.grad........\n",
      "None\n",
      "........b.grad_fn........\n",
      "<AddBackward object at 0x0000021481665CC0>\n"
     ]
    }
   ],
   "source": [
    "# 애는 그라디언트가 필요하지 않아서 None이다.\n",
    "# 얘는 대신에 grad_fn이 있는데 채워진 내용 보면 addBackward 제로다.\n",
    "# b가 a+2 이라 add 연산에 대한 backward를 했다.\n",
    "print(\"........b.data........\")\n",
    "print(b.data)\n",
    "print(\"........b.grad........\")\n",
    "print(b.grad)\n",
    "print(\"........b.grad_fn........\")\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........c.data........\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "........c.grad........\n",
      "None\n",
      "........c.grad_fn........\n",
      "<PowBackward0 object at 0x0000021481665DD8>\n"
     ]
    }
   ],
   "source": [
    "# C는 파워 연산을 해서 그렇다\n",
    "# powerBackward 제로다\n",
    "\n",
    "print(\"........c.data........\")\n",
    "print(c.data)\n",
    "print(\"........c.grad........\")\n",
    "print(c.grad)\n",
    "print(\"........c.grad_fn........\")\n",
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........out.data........\n",
      "tensor(36.)\n",
      "........out.grad........\n",
      "None\n",
      "........out.grad_fn........\n",
      "<SumBackward0 object at 0x00000214816650B8>\n"
     ]
    }
   ],
   "source": [
    "# out은 여기다가 Sum에 대한 backward를 진행했다고 담긴다.\n",
    "print(\"........out.data........\")\n",
    "print(out.data)\n",
    "print(\"........out.grad........\")\n",
    "print(out.grad)\n",
    "print(\"........out.grad_fn........\")\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "# z = 3xX^2 다. \n",
    "# round x분에 round z는 6xX다. 거기다 x가 1일 때 6이지 않나 싶다.\n",
    "x=torch.ones(3, requires_grad=True)\n",
    "y = (x**2)\n",
    "z = y*3\n",
    "print(z)\n",
    "grad = torch.Tensor([0.1,1,10]) \n",
    "# 그 이유는 여기다. 텐서를 벡워드 칸에 맞춰서 넣어주면\n",
    "# 벡워드 되는 이 값에 곱해져서 채워지게 된다.\n",
    "z.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........x.data........\n",
      "tensor([1., 1., 1.])\n",
      "........x.grad........\n",
      "tensor([ 0.6000,  6.0000, 60.0000])\n",
      "........x.grad_fn........\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"........x.data........\")\n",
    "print(x.data)\n",
    "print(\"........x.grad........\")\n",
    "print(x.grad)\n",
    "print(\"........x.grad_fn........\")\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn, torch.nn.functional 에 대해 알아보자\n",
    "\n",
    "* nn 모듈에서 제공하는 기능\n",
    " * parameter, Linear, Containers, Dropout, Conv, Sparse, Pooling, Distance, Loss, Padding, Vision, Non-linear Activation, Data paralell, Normalization, Utilities\n",
    " , Recurrent\n",
    " \n",
    "* nn.functional 에서 제공하는 기능들은\n",
    " * Conv, Pooling, Non-linear activation, Normalization, Linear function, Dropout, Distance, Loss, Vision 등이 있다.\n",
    " \n",
    " \n",
    "## 두개가 지원하는 기능은 유사한데 사용하는 방식의 차이가 있다.\n",
    "\n",
    "### 그러면 nn.Conv2d에 대해 알아보자\n",
    "* torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "* 인풋채널, 아웃풋 채널(=> 이건 사실상 사용하는 필터의 개수)이 있다. \n",
    "* 특징은 Weight값을 직접 선언해주지 않는다. => 나온 변수를 진행하면 컨볼루션 위한 weight를 선언해준다.\n",
    "\n",
    "### nn.functional 에 대해서 보자\n",
    "* torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor\n",
    "* 앞서 nn.con2d에서 input 채널과 out채널을 썻던것과 달리 인풋과 weight 자체를 직접 넣어줘야 한다. 그래서 이거 쓸려면 외부에서 만든 필터를 직접 만들어서 넣어야 한다.\n",
    "#### 그 외 채워주는 변수들은 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 먼저 F(nn.functional)를 사용한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Functional를 쓰려면 Weight를 선언해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,7,7, requires_grad=True)\n",
    "filter = torch.ones(1,1,3,3) # => 얘가 weight이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.]]]], requires_grad=True)\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9., 9., 9., 9., 9.],\n",
      "          [9., 9., 9., 9., 9.],\n",
      "          [9., 9., 9., 9., 9.],\n",
      "          [9., 9., 9., 9., 9.],\n",
      "          [9., 9., 9., 9., 9.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(input, filter)\n",
    "print(out) #=> 원하는 형태의 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9.]]]], grad_fn=<AdaptiveMaxPool2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.AdaptiveMaxPool2d((1, 1))\n",
    "sp=m(out)\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-dbd656e5eb6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Backward 한다라는 것 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(out.grad_fn) # Backward 한다라는 것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,3,3, requires_grad=True)\n",
    "filter = filter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[18.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "<ThnnConv2DBackward object at 0x00000214816733C8>\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(input, filter)\n",
    "print(out)\n",
    "out.backward()\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----input.grad-----\n",
      "tensor([[[[2., 2., 2.],\n",
      "          [2., 2., 2.],\n",
      "          [2., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"----input.grad-----\")\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그 다음에 nn.Conv2d 사용한 것 \n",
    "* 이러한 conv2d는 Bias 값을 가지고 있고 이 Bias 값이 추가가 되어야 저값이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,1,3,3, requires_grad=True)\n",
    "func = nn.Conv2d(1,1,3)\n",
    "        # 첫번째 것은 인풋채널, 두번째는 아웃풋 채널, 세번째는 커널 사이즈\n",
    "        # 필터의 크기는 3x3이다. 라는 얘기다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0629, -0.2306, -0.2874],\n",
       "          [ 0.2578,  0.3079,  0.1300],\n",
       "          [ 0.2097, -0.1708,  0.0922]]]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.weight # => weight 값을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3716, grad_fn=<SumBackward0>)\n",
      "tensor([0.5265], grad_fn=<ThAddBackward>)\n",
      "tensor([[[[0.5265]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 이러한 conv2d는 Bias 값을 가지고 있고 이 Bias 값이 추가가 되어야 저값이 나온다.\n",
    "out = func(input)\n",
    "print(func.weight.sum()) # 바이어스 넣기 전\n",
    "print(func.weight.sum()+func.bias) # 바이어스가 자동으로 붙는 다는 것을 알 수 있다.\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0629, -0.2306, -0.2874],\n",
      "          [ 0.2578,  0.3079,  0.1300],\n",
      "          [ 0.2097, -0.1708,  0.0922]]]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "# 저기의 그라이언트는 다 1과 곱해졌기 때문에\n",
    "# 1에 대한 그라디언트를 게산하면 필터 자체만 나온다.\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight를 직접 설정하는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1196, -0.1307, -0.3295],\n",
      "          [-0.0057, -0.2754,  0.3155],\n",
      "          [ 0.1846, -0.1475,  0.1283]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones(1,1,5,5, requires_grad=True)\n",
    "filter = nn.Conv2d(1,1,3, bias=None)\n",
    "print(filter.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter.weight = torch.nn.Parameter(torch.ones(1,1,3,3) + 1) \n",
    "# 이렇게 weight 값을 직접 설정가능\n",
    "# 나중에 레이어 만들거나 다른 네트워크에서 프리트레인 웨이트 넣을 때 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[18., 18., 18.],\n",
      "          [18., 18., 18.],\n",
      "          [18., 18., 18.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = filter(input) # => 이렇게 한건 구한 weight에다가 우리의 인풋을 넣은 것\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU, Sigmoid, tanh, Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4200, -0.9965, -0.3802],\n",
      "          [-0.3311,  0.3332,  1.5521],\n",
      "          [-1.1324, -0.6175,  0.6984]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "act_input = torch.randn(1,1,3,3,requires_grad=True)\n",
    "print(act_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3332, 1.5521],\n",
      "          [0.0000, 0.0000, 0.6984]]]], grad_fn=<ReluBackward>)\n"
     ]
    }
   ],
   "source": [
    "act = F.relu(act_input)\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3332, 1.5521],\n",
      "          [0.3332, 1.5521]]]], grad_fn=<MaxPool2DWithIndicesBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.MaxPool2d(2, stride=1)\n",
    "m_out = m(act)\n",
    "print(m_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5825, 0.8252],\n",
      "          [0.5825, 0.8252]]]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "act2 = torch.sigmoid(m_out)\n",
    "print(act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Network\n",
    "* class 선언으로 사용하고자 하는 Network를 직접 구현할 수 있음\n",
    "\n",
    "* class로 선언할 Network에 필수 요소는?\n",
    "\n",
    " * > def __init__(self):\n",
    " * > def forward(self, x): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.Max_pool = nn.MaxPool2d(2, stride=1)\n",
    "        self.Avg_pool = nn.AvgPool2d(2, stride=1)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(x)\n",
    "        x = self.Max_pool(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.Avg_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6177]]]], grad_fn=<AvgPool2DBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model() # 모델을 통과하고 나온 value 값으로\n",
    "out(act_input) # act_input이 값이 model 안에 들어가서 \n",
    "              # forward 연산을 타고 나는 결과가 보는 것과 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
