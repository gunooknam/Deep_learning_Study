{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data의 사용법\n",
    "numpy package를 이용하여 간단하게 data에 해당하는 X, target에 해당하는 y를 생성하여 tf.data package의 각종 module, function을 이용한다. epoch 마다 validation data에 대해서 validation을 하는 상황을 가정\n",
    "\n",
    "\n",
    "### Dataset 생성 -> Iterator 생성 -> Dataset 사용 순으로 처리함\n",
    "\n",
    "\n",
    "# Template \n",
    "for문을 활용하여 model training시 data pipeline으로 아래의 function과 method를 사용하는 방법에 대한 예시\n",
    "* Dataset class\n",
    " * tf.data.Dataset.from_tensor_slices로 Dataset class의 instance 새성\n",
    "   * train data에 대한 Dataset class의 instance => tr_data\n",
    "   * validation data에 대한 Dataset class의 instance => val_data\n",
    " * 아래와 같은 method를 활용하여 training 시 필요한 요소를 지정\n",
    "   * instance의 shuffle method를 활용하면 shuffling\n",
    "   * instance의 batch method를 활용하여, batch size 지정\n",
    "   * for 문으로 전체 epoch를 control하므로 repeat method는 활용하지 않음\n",
    "* Iterator class\n",
    " * Dataset class의 instance에서 make_initializable_iterator method로 iterator class의 instance를 생성\n",
    "   * train data에 대한 iterator class의 instance, tr_iterator\n",
    "   * validation data에 대한 iterator class의 instance, val_iterator\n",
    "   * 주의사항으로 make_initializable_iterator method로 iterator class의 instance를 생성하는 경우, random_sees를 고정하지 않는다. \n",
    "     * random_seed를 고정하는 경우, 서로 다른 epoch의 step 별 mini-batch의 구성이 완전히 같아진다.\n",
    "   * Anonymous iterator를 tf.data.Iterator.from_string_handle로 생성한다. \n",
    "     * string_handle argument에 tf.placeholder를 사용한다.\n",
    "       * tr_iterator를 사용할지 val_iterator를 활용할지 조절\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [ 1  1]\n",
      " [ 2  2]\n",
      " [ 3  3]\n",
      " [ 4  4]\n",
      " [ 5  5]\n",
      " [ 6  6]\n",
      " [ 7  7]\n",
      " [ 8  8]\n",
      " [ 9  9]\n",
      " [10 10]\n",
      " [11 11]] (12, 2) (12,)\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터의 개수가 12개인 임의의 데이터셋 생성\n",
    "# c_는 각각의 1 D-array의 column을 하나씩 빼서 [ ]에 담음  \n",
    "X = np.c_[np.arange(12), np.arange(12)]\n",
    "y = np.arange(12)\n",
    "\n",
    "print(X, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2) (8,)\n",
      "(4, 2) (4,)\n"
     ]
    }
   ],
   "source": [
    "# 위의 데이터를 train, validation set으로 split\n",
    "X_tr=X[:8]\n",
    "y_tr=y[:8]\n",
    "\n",
    "X_val = X[8:]\n",
    "y_val = y[8:]\n",
    "\n",
    "print(X_tr.shape, y_tr.shape)\n",
    "print(X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3, batch_size : 2, total_steps : 4\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 3\n",
    "batch_size = 2\n",
    "\n",
    "# 전체 step 수 : training set / batch_size\n",
    "total_steps = int(X_tr.shape[0]/ batch_size)\n",
    "print('epoch : {}, batch_size : {}, total_steps : {}'.format(n_epoch, batch_size, total_steps ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 2), (?,)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 2), (?,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "tr_data = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "# from_tensor_slices\n",
    "# dataset instance를 만들고 \n",
    "tr_data = tr_data.shuffle(buffer_size = 30)\n",
    "tr_data = tr_data.batch(batch_size = batch_size)\n",
    "# 마찬가지로 from_tensor_slices를 통해서 validation data의\n",
    "# instance를 만들고 그 인스턴스를 batch 설정\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_data = val_data.batch(batch_size = batch_size)\n",
    "\n",
    "print(tr_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 방법은 Placeholder로 Dataset 객체를 만드는 경우이다.\n",
    "make_initializable_iterator() 만들어야 함 feed_dict를 선택적으로 하므르써 train data, val data 나눌 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder 를 사용하여 Dataset을 생성한 경우\n",
    "\n",
    "tr_iterator = tr_data.make_initializable_iterator()\n",
    "val_iterator = val_data.make_initializable_iterator()\n",
    "\n",
    "#####\n",
    "handle = tf.placeholder(dtype=tf.string)\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_string_handle( string_handle = handle,\n",
    "             output_shapes = tr_iterator.output_shapes,\n",
    "             output_types = tr_iterator.output_types)\n",
    "    \n",
    "X_mb, y_mb = iterator.get_next() # iteration 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 출력으로 batch_size = 2 로서 2개씩 나오는 것을 확인할 수 있다.\n",
    "train_set에 대한 iteration, validation set에 대한 iteration\n",
    "\n",
    "* 자주 사용하는 형태\n",
    "```\n",
    "while True:\n",
    "    try: \n",
    "        께속 다음값으로 넘어갈 수 있을 때\n",
    "    except: \n",
    "        끝에 도달하면\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 training start\n",
      "step : 1\n",
      "feature: [6 6] label 6\n",
      "feature: [5 5] label 5\n",
      "step : 2\n",
      "feature: [7 7] label 7\n",
      "feature: [1 1] label 1\n",
      "step : 3\n",
      "feature: [3 3] label 3\n",
      "feature: [2 2] label 2\n",
      "step : 4\n",
      "feature: [0 0] label 0\n",
      "feature: [4 4] label 4\n",
      "epoch : 1 training finished\n",
      "at epoch L 1. validation start\n",
      "step : 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step : 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n",
      "\n",
      "\n",
      "\n",
      "epoch : 2 training start\n",
      "step : 1\n",
      "feature: [1 1] label 1\n",
      "feature: [5 5] label 5\n",
      "step : 2\n",
      "feature: [2 2] label 2\n",
      "feature: [6 6] label 6\n",
      "step : 3\n",
      "feature: [0 0] label 0\n",
      "feature: [4 4] label 4\n",
      "step : 4\n",
      "feature: [3 3] label 3\n",
      "feature: [7 7] label 7\n",
      "epoch : 2 training finished\n",
      "at epoch L 2. validation start\n",
      "step : 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step : 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n",
      "\n",
      "\n",
      "\n",
      "epoch : 3 training start\n",
      "step : 1\n",
      "feature: [1 1] label 1\n",
      "feature: [4 4] label 4\n",
      "step : 2\n",
      "feature: [7 7] label 7\n",
      "feature: [2 2] label 2\n",
      "step : 3\n",
      "feature: [6 6] label 6\n",
      "feature: [3 3] label 3\n",
      "step : 4\n",
      "feature: [5 5] label 5\n",
      "feature: [0 0] label 0\n",
      "epoch : 3 training finished\n",
      "at epoch L 3. validation start\n",
      "step : 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step : 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config = sess_config)\n",
    "\n",
    "                                # 이렇게 training 데이터를 뽑는 case, validation 데이터를 뽑는 case로 나눈다.\n",
    "tr_handle, val_handle = sess.run([tr_iterator.string_handle(), \n",
    "                                  val_iterator.string_handle()])\n",
    "\n",
    "\n",
    "# 2 개의 batch_size만큼 뽑겠다.\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    print('epoch : {} training start'.format(epoch+1))\n",
    "    sess.run(tr_iterator.initializer)\n",
    "    n_tr_step = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            n_tr_step+=1\n",
    "            X_tmp, y_tmp = sess.run([X_mb, y_mb], feed_dict = {handle: tr_handle})\n",
    "            print('step : {}'.format(n_tr_step))\n",
    "            #print(X_tmp, y_tmp)\n",
    "            for f, l in zip(X_tmp, y_tmp):\n",
    "                print(\"feature: {} label {}\".format(f,l))\n",
    "        \n",
    "        except:\n",
    "            print('epoch : {} training finished'.format(epoch+1))\n",
    "            break\n",
    "        \n",
    "    print('at epoch L {}. validation start'.format(epoch + 1))\n",
    "    sess.run(val_iterator.initializer)\n",
    "        \n",
    "    n_val_step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            n_val_step +=1\n",
    "            X_tmp, y_tmp = sess.run([X_mb, y_mb], feed_dict = {handle: val_handle})\n",
    "                \n",
    "            print('step : {}'.format(n_val_step))\n",
    "            print(X_tmp, y_tmp)\n",
    "        except:\n",
    "            print('validation finished')\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipeline\n",
    "\n",
    "## 1. tf.data.Dataset 이라는 코드를 정의<br>\n",
    "* 메모리 안의 있는 몇몇 tensor들로부터 Dataset을 만들기위해 다음과 같은 method 사용\n",
    "  * tf.data.TextLineDataset(filenames)\n",
    "  * tf.data.FixedLengthRecordDataset(filenames)\n",
    "  * tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "## 2. Transformation \n",
    "* Dataset.map()\n",
    "* Dataset.batch() : batch 사이즈를 정하는 것\n",
    "* Dataset.shuffle() : 섞는 것\n",
    "\n",
    "### 3. Iterator\n",
    "* Iterator.initializer: iterator의 상태를 intialize한다.\n",
    "* iterator.get_next() : 다음 iterator를 얻어오는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ● placeholder를 쓰지 않고 Dataset 객체 할당하는 경우\n",
    " make_one_shot_iterator를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# 일반적인 방법\n",
    "# 요것은 generator이다. \n",
    "# 리스트를 하나씩 yield 한다! \n",
    "def generator():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "\n",
    "        # yield를 한 generator가 생성한 값이 텐서플로우에 생성\n",
    "dataset = tf.data.Dataset.from_generator(generator, tf.float32)\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\n",
    "        # 완전 iterator 개념! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(10): # 횟수가 넘어가면 에러가 뜬다.\n",
    "        _data = sess.run(dataset)\n",
    "        print(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 0.0\n",
      "11.0 1.0\n",
      "12.0 2.0\n",
      "13.0 3.0\n",
      "14.0 4.0\n",
      "15.0 5.0\n",
      "16.0 6.0\n",
      "17.0 7.0\n",
      "18.0 8.0\n",
      "19.0 9.0\n"
     ]
    }
   ],
   "source": [
    "# 우리는 데이터셋을 만들 때 라벨과 feature를 만드는데 \n",
    "def generator():\n",
    "    for i, j in zip(range(10, 20), range(10)):\n",
    "        yield (i, j)\n",
    "        \n",
    "# yield를 한 generator가 생성한 값이 텐서플로우에 생성\n",
    "# 두개를 yield 할 때는 ( , ) 튜플로 넣어주자.\n",
    "# 그 타입으로 받아 와라라는 말\n",
    "dataset = tf.data.Dataset.from_generator(generator, (tf.float32,tf.float32))\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\n",
    "        # 완전 iterator 개념! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(10): # 횟수가 넘어가면 에러가 뜬다.\n",
    "        _label, _feat = sess.run(dataset)\n",
    "        print(_label, _feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터가 매우 많을 때 1000개?\n",
    "그것에 대한 미니배치를 쉽게 할 수 있다.\n",
    "\n",
    ".batch(batch_size) 함수를 이용하여 지정 <br>\n",
    ".shuffle로 데이터를 섞을 수 있다. 인자는 셔플 사이즈이다.\n",
    "(셔플하려면 일정량의 통을 가지고 있어야 하니까 통 사이즈)\n",
    "얼마만큼 큐에다 넣고 섞을 지 지정한 것 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[830. 831. 832. 833. 834. 835. 836. 837. 838. 839. 840. 841. 842. 843.\n",
      " 844. 845. 846. 847. 848. 849.] [820. 821. 822. 823. 824. 825. 826. 827. 828. 829. 830. 831. 832. 833.\n",
      " 834. 835. 836. 837. 838. 839.]\n",
      "[850. 851. 852. 853. 854. 855. 856. 857. 858. 859. 860. 861. 862. 863.\n",
      " 864. 865. 866. 867. 868. 869.] [840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852. 853.\n",
      " 854. 855. 856. 857. 858. 859.]\n",
      "[770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789.] [760. 761. 762. 763. 764. 765. 766. 767. 768. 769. 770. 771. 772. 773.\n",
      " 774. 775. 776. 777. 778. 779.]\n"
     ]
    }
   ],
   "source": [
    "# 우리는 데이터셋을 만들 때 라벨과 feature를 만드는데 \n",
    "\n",
    "def generator():\n",
    "    for i, j in zip(range(10, 1100), range(1000)):\n",
    "        yield (i, j)\n",
    "        \n",
    "# yield를 한 generator가 생성한 값이 텐서플로우에 생성\n",
    "# 두개를 yield 할 때는 ( , ) 튜플로 넣어주자.\n",
    "# 그 타입으로 받아 와라라는 말\n",
    "dataset = tf.data.Dataset.from_generator(generator, (tf.float32,tf.float32))\\\n",
    "                         .batch(20)\\\n",
    "                         .shuffle(7777)\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\\\n",
    "        # 완전 iterator 개념! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(3): # 횟수가 넘어가면 에러가 뜬다.\n",
    "        # 20개씩 세트로 리턴된다.\n",
    "        _label, _feat = sess.run(dataset)\n",
    "        print(_label, _feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위의 결과는 배치 내부끼리는 안 섞이고 다른 배치들과는 섞이는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제너레이터 하나 정의하고 쓰는 것이 굉장히 편하다. 이미지 같은 것을 generator로 yield해서 전달하는 것을 구현할 수도 있다. \n",
    "\n",
    "하지만 이렇게 직접 정의하는 generator 가지고는 병목이 일어날 수 있다. API를 제공하더라도 generator에 의존하니까!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] 1\n"
     ]
    }
   ],
   "source": [
    "# 우리는 데이터셋을 만들 때 라벨과 feature를 만드는데 \n",
    "\n",
    "def generator():\n",
    "    for i, j in zip(range(10, 1100), range(1000)):\n",
    "        yield (i, j)\n",
    "        \n",
    "# yield를 한 generator가 생성한 값이 텐서플로우에 생성\n",
    "# 두개를 yield 할 때는 ( , ) 튜플로 넣어주자.\n",
    "# 그 타입으로 받아 와라라는 말\n",
    "\n",
    "#           TextLineDataset이라는 것도 있다. \n",
    "dataset = tf.data.TextLineDataset('./up_down_dataset.csv')\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\\\n",
    "        # 완전 iterator 개념! \n",
    "\n",
    "\n",
    "# 결과는 String 형으로 파일을 읽어올 수 있다. LinebyLine\n",
    "# 얘를 가져다가 디코딩을 해줄 수가 있다. \n",
    "\n",
    "\n",
    "# csv 파일에서는 missing value, 즉 빠진 값이 존재할 수 있는데\n",
    "# 그것은 멀로 채울 것이냐? 에 대한 정의 record_default \n",
    "\n",
    "\n",
    "lines = tf.decode_csv(dataset,\n",
    "                      record_defaults=[[0]]*11)\n",
    "feature = tf.stack(lines[1:]) # 배치의 셋팅을 안했을 때\n",
    "                              # 축을 넣지 않는다.\n",
    "#feature = tf.stack(lines[1:], axis=1)\n",
    "label = lines[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    _feat,_lab =sess.run([feature, label])\n",
    "    print(_feat, _lab )\n",
    "#     _feat, _lab = sess.run([feature, label])\n",
    "#     print(_feat, _lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하나짜리는 별 신경 안써도 되지만 배치로 합칠 때는 axis를 고려해야 한다.\n",
    "numpy에는 vstack이 있는데 tf에는 axis 잘따져야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset('./up_down_dataset.csv')\\\n",
    "                         .batch(20)\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\\\n",
    "\n",
    "lines = tf.decode_csv(dataset,\n",
    "                      record_defaults=[[0]]*11)\n",
    "feature = tf.stack(lines[1:], axis=1)\n",
    "label = lines[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    _feat,_lab = sess.run([feature, label])\n",
    "    for f, l in zip(_feat, _lab):\n",
    "        print(l,f) # 1:1 매칭 시켜준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset('./up_down_dataset.csv')\\\n",
    "                         .batch(20)\\\n",
    "                         .make_one_shot_iterator()\\\n",
    "                         .get_next()\\\n",
    "\n",
    "lines = tf.decode_csv(dataset,\n",
    "                      record_defaults=[[0]]*11) # [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
    "# lines는 아래와 같은 배열이 들어있다.\n",
    "'''\n",
    " 그래서 세로로 배치 사이즈 만큼 원소가 잡힌다. 20개\n",
    "\n",
    " lines[0] = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "[array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]), #원소 20개\n",
    " array([0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9]),\n",
    " array([1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8]),\n",
    " array([2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7]),\n",
    " array([3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6]),\n",
    " array([4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]),\n",
    " array([5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4]),\n",
    " array([6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3]),\n",
    " array([7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2, 7, 2]),\n",
    " array([8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1, 8, 1]),\n",
    " array([9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0])]\n",
    " \n",
    " '''\n",
    "\n",
    "feature = tf.stack(lines[1:], axis=1)\n",
    "label = lines[0]\n",
    "\n",
    "# feature가 우리가 예상한 batch size의 출력으로 나온다.\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        _feat,_lab = sess.run([feature, label])\n",
    "        print(_feat.shape, _lab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48818791 0.65980126]\n",
      "[0.34238975 0.98514686]\n",
      "[0.7840512  0.59899738]\n",
      "[0.0241973  0.55126401]\n",
      "[0.0772997  0.95676098]\n",
      "[0.19067756 0.71681624]\n",
      "[0.021248   0.04618403]\n",
      "[0.1549198  0.17103496]\n",
      "[0.67748982 0.47563405]\n",
      "[0.32795494 0.17532543]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.sample((10, 2))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_element))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
