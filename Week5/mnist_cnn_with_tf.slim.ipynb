{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks\n",
    "\n",
    "### A LeNet-5 like cnn MNIST classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "######################\n",
    "#  Slim을 사용할 것  #\n",
    "slim = tf.contrib.slim\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "np.random.seed(777)\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  3\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 10000\n",
    "print(\"label = \",train_labels[index])\n",
    "print(train_data[index].shape)\n",
    "plt.imshow(train_data[index] , cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "#for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 10000)\n",
    "test_dataset = test_dataset.batch(batch_size = len(test_data))\n",
    "print(test_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle \\\n",
    "                           (string_handle=handle, \\\n",
    "                            output_types=train_dataset.output_types, \\\n",
    "                            output_shapes=train_dataset.output_shapes) \n",
    "\n",
    "x, y = iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 TensorFlow 그래프는 수천 개에 이르는 많은 노드를 가질 수 있고 한 눈에 쉽게 보거나 보통의 그래프 도구를 이용해 그리기에는 너무 많음...\n",
    "\n",
    "그래서 변수의 이름을 그룹으로 묶어서(name scoping) 계층화하는 방법을 사용할 수 있다.\n",
    "\n",
    "처음에는 계층의 최상단에 있는 이름들만 보여지다가 노드를 클릭해서 펼치면 볼 수 있다.\n",
    "\n",
    "여기 tf.name_scope를 사용해 \"hidden\"이라는\n",
    "name scope 아래에 세 가지 기능을 정의한 예가 있습니다\n",
    "\n",
    "```c\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope('hidden') as scope:\n",
    "  a = tf.constant(5, name='alpha')\n",
    "  W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights')\n",
    "  b = tf.Variable(tf.zeros([1]), name='biases')\n",
    " ```\n",
    "  \n",
    "  위의 예시 코드의 결과로 아래와 같은 세 가지 연산(op)의 이름이 나오고\n",
    "* 'hidden/alpha'\n",
    "* 'hidden/weights'\n",
    "* 'hidden/biases'\n",
    "\n",
    "와 같이 확인이 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "slim.conv2d()\n",
    "tf.contrib.layers.conv2d(\n",
    "    inputs,\n",
    "    num_outputs,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding='SAME',\n",
    "    data_format=None,\n",
    "    rate=1,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=initializers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "    # Mnist라서 LeNet을 쓰지만 입력 크기는 다르다.\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1,28,28,1])\n",
    "        \n",
    "    # Convolutional Layer #1\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = slim.conv2d(x_image, 32, [5,5], scope='conv1')\n",
    "    #conv1 = tf.layers.conv2d(\n",
    "    #    inputs=x_image,\n",
    "    #    filters=32,\n",
    "    #    kernel_size=[5, 5],\n",
    "    #    padding=\"same\",\n",
    "    #    activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n",
    "    #pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional Layer #2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = slim.conv2d(pool1, 64, [5,5], scope='conv2')\n",
    "    \n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = slim.max_pool2d(conv2,  [2,2], scope='pool2')\n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = slim.flatten(pool2, scope='flatten')\n",
    "    \n",
    "    \n",
    "    # 7 * 7 * 64 = 3136\n",
    "    # (1 x 3136 ) (3136 X 1024 )\n",
    "    # Fully connected Layer\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    fc1 = slim.fully_connected(pool2_flat, 1024, scope='fc1')\n",
    "    #dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    fc1_drop = slim.dropout(fc1, keep_prob=0.6, is_training=is_training, scope='dropout')\n",
    "    #dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=is_training)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = slim.fully_connected(fc1_drop, 10, activation_fn=None, scope='logits')\n",
    "  \n",
    "    return logits, is_training, x_image\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model을 빌드함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, is_training, x_image = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "#y_one_hot = tf.one_hot(y, depth=10)\n",
    "#cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot, logits=logits)\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y,\n",
    "                                                       logits=logits)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지정 tf.summary.FileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: ./graphs/train/\n"
     ]
    }
   ],
   "source": [
    "train_dir = './graphs/train/'\n",
    "print('Saving graph to: %s' % train_dir)\n",
    "# 1. writer 객체 생성\n",
    "train_writer = tf.summary.FileWriter(train_dir)\n",
    "\n",
    "# 2. 그래프를 writer에 넣는다.\n",
    "train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    # 3. tf.summary로  board에다가 그릴 것 넣기\n",
    "    # tf.summary.scalar => loss 같은것\n",
    "    # tf.summary.scalar => image\n",
    "    # tf.summary.histogram => matrix\n",
    "    tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "    tf.summary.image('images', x_image)\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "    # 4.  merge all summaries\n",
    "    # 5. 넣은 것 all summary 하기\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.train.Saver\n",
    "* tf.train.Saver.save(sess, save_path, global_step=None...)\n",
    "* tf.train.Saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Session() and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. writer 객체 생성\n",
    "# 2. 그래프를 writer에 넣는다.\n",
    "# 3. tf.summary로  board에다가 그릴 것 넣기\n",
    "# 4. merge all summaries\n",
    "# 5. 넣은 것 all summary 하기\n",
    "# 6. sess.run할 때 summary 넣어주기\n",
    "# 7. sess.run해서 나온 summary를 writer에 추가 , global step 지정   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.291449546813965\n",
      "step: 10, loss: 2.140798568725586\n",
      "step: 20, loss: 2.091951370239258\n",
      "step: 30, loss: 1.8677887916564941\n",
      "step: 40, loss: 1.5232555866241455\n",
      "step: 50, loss: 1.3040505647659302\n",
      "step: 60, loss: 0.8147233724594116\n",
      "step: 70, loss: 0.7721024751663208\n",
      "step: 80, loss: 0.8907058238983154\n",
      "step: 90, loss: 0.7796086668968201\n",
      "step: 100, loss: 0.6180256605148315\n",
      "step: 110, loss: 0.6620495319366455\n",
      "step: 120, loss: 0.5464301109313965\n",
      "step: 130, loss: 0.4604496359825134\n",
      "step: 140, loss: 0.20648175477981567\n",
      "step: 150, loss: 0.2571292221546173\n",
      "step: 160, loss: 0.40053844451904297\n",
      "step: 170, loss: 0.8791305422782898\n",
      "step: 180, loss: 0.25652989745140076\n",
      "step: 190, loss: 0.20736101269721985\n",
      "step: 200, loss: 0.32028090953826904\n",
      "step: 210, loss: 0.3734211325645447\n",
      "step: 220, loss: 0.2915639877319336\n",
      "step: 230, loss: 0.3722003698348999\n",
      "step: 240, loss: 0.5087811350822449\n",
      "step: 250, loss: 0.4688178598880768\n",
      "step: 260, loss: 0.39242827892303467\n",
      "step: 270, loss: 0.17259420454502106\n",
      "step: 280, loss: 0.2225957065820694\n",
      "step: 290, loss: 0.34507137537002563\n",
      "step: 300, loss: 0.24693484604358673\n",
      "step: 310, loss: 0.21674662828445435\n",
      "step: 320, loss: 0.29804590344429016\n",
      "step: 330, loss: 0.2160342037677765\n",
      "step: 340, loss: 0.3036457300186157\n",
      "step: 350, loss: 0.14197573065757751\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 1\n",
    "step = 0\n",
    "\n",
    "\n",
    "for epochs in range(max_epochs):\n",
    "    # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  start_time = time.time()\n",
    "  while True:\n",
    "    try:\n",
    "        # 여기를 직접 채워 넣으시면 됩니다.\n",
    "      _, loss = sess.run([train_step, cross_entropy],\n",
    "                          feed_dict={handle: train_handle,\n",
    "                                    is_training: True})\n",
    "      if step % 10 == 0:\n",
    "        print(\"step: {}, loss: {}\".format(step, loss))\n",
    "        # 6. sess.run할 때 summary 넣어주기\n",
    "        # summary를 sess.run\n",
    "        summary_str = sess.run(summary_op,\n",
    "                               feed_dict={handle: train_handle, is_training: False})\n",
    "        # 7. sess.run해서 나온 summary를 writer에 추가 , global step 지정                        \n",
    "        train_writer.add_summary(summary_str, global_step=step)\n",
    "        \n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"  End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "\n",
    "  # Save a model per every one epoch in periodically\n",
    "  if epochs % 2 == 0:\n",
    "    print(\"    Save model at {} epochs\".format(epochs))\n",
    "    saver.save(sess, train_dir + 'model.ckpt', global_step=step)\n",
    "    \n",
    "  print(\"  Epochs: {} Elapsed time: {}\".format(epochs, time.time() - start_time))\n",
    "  print(\"\\n\")\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index]\n",
    "y_pred = sess.run(logits, feed_dict={x: batch_xs, is_training: False})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "  p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "  p.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
