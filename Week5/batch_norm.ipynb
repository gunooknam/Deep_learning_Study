{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization을 하는 코드\n",
    "출처 - https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-10-6-mnist_nn_batchnorm.ipynb \n",
    "\n",
    "\n",
    "* 1. tf.layers.batch_normalization(net, training=self.mode)\n",
    "\n",
    "* 2.  train 과정에서 moviing_mean과 moving_var이 직접적으로 호출이 안되고 train과 별도로 moving_mean과 moving_var에 대한 op를 실행시켜야 한다. 이걸 update_ops=tf.get_collection(tf.Graphkeys.UPDATE_OPS)가 하니까 이와 같은 op를 넘겨 받아서 sess.run에서 실행한다.\n",
    "\n",
    "\n",
    "\n",
    "            # tf.get_collection(tf.GraphKeys.UPDATA_OPS, scope=none) => BN \n",
    "           update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "           with tf.control_dependencies(update_ops):    # control dependency 추가\n",
    "                    self.train_op = optimizer(lr).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b9007a7f7abb>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\gunooknam\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\gunooknam\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\gunooknam\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\gunooknam\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\gunooknam\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "      \n",
    "            with tf.variable_scope(name):\n",
    "                # Placeholders are defined\n",
    "                self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "                self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
    "                self.mode = tf.placeholder(tf.bool, name='train_mode')   \n",
    "\n",
    "\n",
    "                # Loop over hidden layers\n",
    "                net = self.X\n",
    "                for i, h_dim in enumerate(hidden_dims):\n",
    "                    with tf.variable_scope('layer{}'.format(i)):\n",
    "                        net = tf.layers.dense(net, h_dim)\n",
    "\n",
    "                        if use_batchnorm:\n",
    "                            net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "\n",
    "                        net = activation_fn(net)\n",
    "\n",
    "                # Attach fully connected layers\n",
    "                net = tf.contrib.layers.flatten(net)\n",
    "                net = tf.layers.dense(net, output_dim)\n",
    "\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "                self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
    "\n",
    "                # When using the batchnormalization layers,\n",
    "                # it is necessary to manually add the update operations\n",
    "                # because the moving averages are not included in the graph    \n",
    "\n",
    "                # tf.get_collection(tf.GraphKeys.UPDATA_OPS, scope=none) => BN \n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "                with tf.control_dependencies(update_ops):    # control dependency 추가                 \n",
    "                    self.train_op = optimizer(lr).minimize(self.loss)\n",
    "\n",
    "                # Accuracy etc \n",
    "                softmax = tf.nn.softmax(net, name='softmax')\n",
    "                self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \n",
    "    def __init__(self, sess, model):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        \n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                \n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                \n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0] # 배치 크기 * 구간 loss \n",
    "                total_acc += step_acc * X_batch.shape[0] # 배치 크기 * 구간 정확도\n",
    "            \n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            \n",
    "            return total_loss, total_acc\n",
    "    \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.mode: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "\n",
    "            return self.sess.run([loss, accuracy], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-a83dd5e679c2>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "N = 55000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# We create two models: one with the batch norm and other without\n",
    "bn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\n",
    "nn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n",
    "\n",
    "# We create two solvers: to train both models at the same time for comparison\n",
    "# Usually we only need one solver class\n",
    "bn_solver = Solver(sess, bn)\n",
    "nn_solver = Solver(sess, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.13693(95.81%) vs No Batchnorm Loss(Acc): 0.24393(92.91%)\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.14058(95.78%) vs No Batchnorm Loss(Acc): 0.24739(92.84%)\n",
      "\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.12758(95.82%) vs No Batchnorm Loss(Acc): 0.16997(95.00%)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.14236(96.00%) vs No Batchnorm Loss(Acc): 0.17680(94.62%)\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.08615(97.37%) vs No Batchnorm Loss(Acc): 0.18812(94.51%)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.11321(96.70%) vs No Batchnorm Loss(Acc): 0.19073(94.68%)\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.07394(97.64%) vs No Batchnorm Loss(Acc): 0.16787(94.96%)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.10075(97.16%) vs No Batchnorm Loss(Acc): 0.19988(94.46%)\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.07484(97.65%) vs No Batchnorm Loss(Acc): 0.18691(94.93%)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.10847(97.06%) vs No Batchnorm Loss(Acc): 0.23278(94.46%)\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.06209(98.04%) vs No Batchnorm Loss(Acc): 0.13947(96.06%)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.10311(97.28%) vs No Batchnorm Loss(Acc): 0.16453(95.96%)\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.07168(97.67%) vs No Batchnorm Loss(Acc): 0.18516(94.80%)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.11756(96.82%) vs No Batchnorm Loss(Acc): 0.21988(94.54%)\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.05758(98.15%) vs No Batchnorm Loss(Acc): 0.14190(95.96%)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.10274(97.16%) vs No Batchnorm Loss(Acc): 0.17394(95.46%)\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.05003(98.37%) vs No Batchnorm Loss(Acc): 0.15061(95.82%)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.09566(97.46%) vs No Batchnorm Loss(Acc): 0.22116(94.84%)\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.06047(98.00%) vs No Batchnorm Loss(Acc): 0.12021(96.67%)\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): 0.10554(97.26%) vs No Batchnorm Loss(Acc): 0.19073(95.58%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    for _ in range(N//batch_size):\n",
    "        X_batch, y_batch= mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        _, nn_loss = nn_solver.train(X_batch, y_batch)\n",
    "        \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    \n",
    "    train_losses.append([b_loss, n_loss])\n",
    "    train_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    \n",
    "    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    # Save valid losses/acc\n",
    "    valid_losses.append([b_loss, n_loss])\n",
    "    valid_accs.append([b_acc, n_acc])\n",
    "    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})\\n')\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
