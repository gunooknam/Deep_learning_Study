{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification\n",
    "여러개를 Classification 함, 여기서는 logistic을 쓰진 않을 것임, score값에 softmax를 거치면 확률화를 해줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from cifar10_data_util import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_{j} e^{f_j}})$$\n",
    "\n",
    "${e^{f_{y_i}}}$이거랑 ${\\sum_{j} e^{f_j}}$ 이거는 exponetial이라 갑자기 겁나 커진다. 큰 수를 나누는 것이 unsafety해짐. 그래서 Normalization씀 이거는 계산 트릭인데 exponential한 계산을 다룰 때 꼭 써야함\n",
    "\n",
    "$$logC=−max_jf_j$$ .\n",
    "\n",
    "$$아래처럼 Normalization$$\n",
    "\n",
    "$$\\frac{Ce^{f_{y_i}}}{C\\sum_{j} e^{f_j}}= \\frac{e^{f_{y_i}+logC }}{\\sum_{j} e^{f_j+logC }}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(X, W, y, reg): # 더 빠른 연산이 존재\n",
    "#inputs:\n",
    "# - X : (N,D)\n",
    "# - W : (D,C)\n",
    "#y값은 onehot에서 추출된 상태 => Cifar data를 쓸것이기 때문에\n",
    "# - y : (N,) \n",
    "# - reg : (float) regularization strength\n",
    "#   Li = -log(e^fyi/∑je^fj)\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        scores = np.dot(X[i],W)\n",
    "        shift_scores = scores - max(scores)\n",
    "        \n",
    "        loss_i = np.log(sum(np.exp(shift_scores))) -shift_scores[y[i]]\n",
    "        loss += loss_i\n",
    "        \n",
    "        for j in range(num_classes):\n",
    "            softmax_output = np.exp( shift_scores[j])/sum(np.exp(shift_scores) )\n",
    "            if j==y[i]:\n",
    "                dW[:,y[i]] += (-1 + softmax_output)*X[i]\n",
    "            else:\n",
    "                dW[:,j] += softmax_output*X[i]\n",
    "    \n",
    "    loss /= num_train\n",
    "    loss += 0.5 * reg * np.sum(W*W)\n",
    "    dW = dW/num_train + reg* W\n",
    "    #  W - (learning_rate)*dW \n",
    "    #  reg* W라는 텀이 추가로 더 붙어서 Weight가 더 까임\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask] # 49000~50000까지 val \n",
    "    \n",
    "    mask = list(range(num_training)) # 49000까지 train_set\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    mask = list(range(num_test)) # test set 1000\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    # -1값을 주면 지멋대로 element 개수 맞춰서 reshape\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.398645\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "loss, grad = softmax_loss_naive(X_dev, W, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predit(X, W):\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    scores=X.dot(W)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]/[1000] loss is :  2.398645070764766\n",
      "accuracy :  0.264\n",
      "[100]/[1000] loss is :  58.69969204185975\n",
      "accuracy :  0.702\n",
      "[200]/[1000] loss is :  0.12309558728507125\n",
      "accuracy :  0.994\n",
      "[300]/[1000] loss is :  5.015792823739109e-05\n",
      "accuracy :  1.0\n",
      "[400]/[1000] loss is :  4.342502281254835e-05\n",
      "accuracy :  1.0\n",
      "[500]/[1000] loss is :  4.0776202783114726e-05\n",
      "accuracy :  1.0\n",
      "[600]/[1000] loss is :  3.93303906573008e-05\n",
      "accuracy :  1.0\n",
      "[700]/[1000] loss is :  3.840774750409443e-05\n",
      "accuracy :  1.0\n",
      "[800]/[1000] loss is :  3.776163035016607e-05\n",
      "accuracy :  1.0\n",
      "[900]/[1000] loss is :  3.728041661094423e-05\n",
      "accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(X_dev,W, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_itr=1000\n",
    "for i in range(max_itr):\n",
    "    loss_naive, grad_naive = softmax_loss_naive(X_dev, W, y_dev, 0.000005)\n",
    "    W = W-learning_rate*grad_naive\n",
    "    y_dev_pred = predit(X_dev, W)\n",
    "    val_accuracy = np.mean(y_dev == y_dev_pred)\n",
    "    \n",
    "    if i% 100==0:\n",
    "        print(\"[%d]/[%d] loss is : \" %( i,max_itr) ,loss_naive)\n",
    "        print(\"accuracy : \", val_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
