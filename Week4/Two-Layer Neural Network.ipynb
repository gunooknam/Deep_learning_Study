{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Implementation of training a 2-layer Neural Network\n",
    "\n",
    "## ● Forward Propagation & Backword Propagation & Gradient Descent Example\n",
    "-------------------\n",
    "\n",
    "* Fordward Pass\n",
    " * 이 타이밍에 값을 저장한다. \n",
    "* Backword Pass\n",
    " * Forward Pass 때 저장한 값을 사용한다. Current Gradient = Upstream_gradient * Local_gradient\n",
    " * 단 Vectorized 연산이므로 Muliplication 연산 시 Shape을 무조건 맞춰주자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 / loss :  34571.94586990758\n",
      "iteration:  100 / loss :  5884.235240193674\n",
      "iteration:  200 / loss :  2372.6068770883826\n",
      "iteration:  300 / loss :  1435.4394943673965\n",
      "iteration:  400 / loss :  999.3595010513415\n",
      "iteration:  500 / loss :  771.1232100869075\n",
      "iteration:  600 / loss :  629.2433043407513\n",
      "iteration:  700 / loss :  496.2962119208562\n",
      "iteration:  800 / loss :  430.18564811233205\n",
      "iteration:  900 / loss :  358.1158100662244\n",
      "iteration:  1000 / loss :  347.867775838538\n",
      "iteration:  1100 / loss :  306.947839959168\n",
      "iteration:  1200 / loss :  289.02855329225844\n",
      "iteration:  1300 / loss :  264.652195744941\n",
      "iteration:  1400 / loss :  248.78350216722026\n",
      "iteration:  1500 / loss :  237.50544522160862\n",
      "iteration:  1600 / loss :  221.09892610453386\n",
      "iteration:  1700 / loss :  200.96270593483945\n",
      "iteration:  1800 / loss :  187.1119736230868\n",
      "iteration:  1900 / loss :  169.60634277541786\n",
      "iteration:  2000 / loss :  158.10388984425566\n",
      "iteration:  2100 / loss :  162.22987704389993\n",
      "iteration:  2200 / loss :  154.7740574527822\n",
      "iteration:  2300 / loss :  144.09189074203874\n",
      "iteration:  2400 / loss :  133.8666007925558\n",
      "iteration:  2500 / loss :  126.0241035991553\n",
      "iteration:  2600 / loss :  118.41642123072967\n",
      "iteration:  2700 / loss :  114.07503010072236\n",
      "iteration:  2800 / loss :  117.54128603487493\n",
      "iteration:  2900 / loss :  111.033181643603\n",
      "iteration:  3000 / loss :  108.85532341279547\n",
      "iteration:  3100 / loss :  102.87858405080418\n",
      "iteration:  3200 / loss :  93.64116486629194\n",
      "iteration:  3300 / loss :  86.36697511877529\n",
      "iteration:  3400 / loss :  88.22589835360736\n",
      "iteration:  3500 / loss :  83.46403696058991\n",
      "iteration:  3600 / loss :  82.65593731405524\n",
      "iteration:  3700 / loss :  81.73649454912874\n",
      "iteration:  3800 / loss :  75.69364149902482\n",
      "iteration:  3900 / loss :  70.7793662186685\n",
      "iteration:  4000 / loss :  68.35934742397109\n",
      "iteration:  4100 / loss :  61.24023628214187\n",
      "iteration:  4200 / loss :  61.14775515095103\n",
      "iteration:  4300 / loss :  57.050505062161\n",
      "iteration:  4400 / loss :  54.12607243079818\n",
      "iteration:  4500 / loss :  52.91627186682871\n",
      "iteration:  4600 / loss :  55.05970194617427\n",
      "iteration:  4700 / loss :  51.7221434170726\n",
      "iteration:  4800 / loss :  52.40652892967415\n",
      "iteration:  4900 / loss :  52.485256178379714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "# np.random.randn이라고도 쓸 수 있으나 귀찬..\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X, Y = randn(N, D_in), randn(N, D_out) # Training data 그냥 random initialize \n",
    "W1, W2 = randn(D_in, H), randn(H,D_out)\n",
    "#--------------------------------------\n",
    "# X => (64, 1000)\n",
    "# W1 => (1000,100)\n",
    "# X * W1 => (64,100) = F\n",
    "# \n",
    "# Hidden = Sigmoid(F)\n",
    "#\n",
    "#--------------------------------------\n",
    "# W2 => (100,10)\n",
    "# Hidden * W2 => (64,10) = Pred\n",
    "# -------------------------------------\n",
    "learning_rate = 1e-4\n",
    "itr = 5000\n",
    "# W1, W2는 randn으로 random normal distribution\n",
    "for t in range(itr):\n",
    "    # Forward pss\n",
    "    F = np.dot(X,W1)\n",
    "    Hidden = 1 / (1 + np.exp( F )) # (64,100)\n",
    "    Y_pred = np.dot(Hidden, W2) # (64,10)\n",
    "    loss = np.square(Y_pred - Y).sum()\n",
    "    if t % 100==0 :\n",
    "        print('iteration: ',t, '/ loss : ',loss)\n",
    "    \n",
    "    # Backword pass\n",
    "    grad_Y_pred = 2.0 * (Y_pred - Y) # loss 미분\n",
    "    \n",
    "                                            # Hidden*W2 라서 이거 미분하면 Hidden \n",
    "    grad_W2 = np.dot(Hidden.T, grad_Y_pred) # local_gradient(Hidden.T) X UpStream_Gradient(grad_Y_pred)\n",
    "    \n",
    "    grad_Hidden = np.dot(grad_Y_pred , W2.T) #UpStrean_Gradient(grad_Y_pred)* local_gradient(W2.T)\n",
    "    \n",
    "    grad_F = grad_Hidden * Hidden * (1-Hidden)\n",
    "    \n",
    "    grad_W1 = np.dot( X.T, grad_F )\n",
    "\n",
    "    W1 -= learning_rate * grad_W1 # vector 연산으로 W1 업데이트\n",
    "    W2 -= learning_rate * grad_W2 # vector 연산으로 W2 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위랑 비슷한 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 / loss :  0.9999420576229583\n",
      "iteration:  100 / loss :  0.8168417543572405\n",
      "iteration:  200 / loss :  0.13413742791683234\n",
      "iteration:  300 / loss :  0.02485797955243002\n",
      "iteration:  400 / loss :  0.012803206501734974\n",
      "iteration:  500 / loss :  0.00843454631343032\n",
      "iteration:  600 / loss :  0.006225278772027247\n",
      "iteration:  700 / loss :  0.0049053405908655754\n",
      "iteration:  800 / loss :  0.004032947335716065\n",
      "iteration:  900 / loss :  0.003415851450959029\n",
      "iteration:  1000 / loss :  0.002957508916213248\n",
      "iteration:  1100 / loss :  0.00260432078115263\n",
      "iteration:  1200 / loss :  0.002324230343381887\n",
      "iteration:  1300 / loss :  0.0020969291129853615\n",
      "iteration:  1400 / loss :  0.0019089472603585245\n",
      "iteration:  1500 / loss :  0.0017510097873727343\n",
      "iteration:  1600 / loss :  0.001616527941228396\n",
      "iteration:  1700 / loss :  0.0015006962303576751\n",
      "iteration:  1800 / loss :  0.001399929751428719\n",
      "iteration:  1900 / loss :  0.0013115013354709657\n"
     ]
    }
   ],
   "source": [
    "A = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ])\n",
    "B = np.array([ [0,1,1,0] ]).T\n",
    "syn0 = 2*np.random.random((3,4))-1\n",
    "syn1 = 2*np.random.random((4,1))-1\n",
    "\n",
    "for j in range(2000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(A,syn0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n",
    "    loss = np.square(B - l2).sum()\n",
    "    if j % 100==0 :\n",
    "        print('iteration: ',j, '/ loss : ',loss)\n",
    "    l2_delta = 2*(B - l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += A.T.dot(l1_delta)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
